# -*- coding: utf-8 -*-
"""machine learning .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mXWke5DdfLQVbRjNbxwwaSLMR46YaGA6
"""

# Candidate Elimination Algorithm

# Training data (EnjoySport example)
data = [
    ['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same', 'Yes'],
    ['Sunny', 'Warm', 'High', 'Strong', 'Warm', 'Same', 'Yes'],
    ['Rainy', 'Cold', 'High', 'Strong', 'Warm', 'Change', 'No'],
    ['Sunny', 'Warm', 'High', 'Strong', 'Cool', 'Change', 'Yes']
]

concepts = [row[:-1] for row in data]
target = [row[-1] for row in data]

# Initialize S and G
S = ['ϕ'] * len(concepts[0])
G = [['?' for _ in range(len(concepts[0]))]]

for i, instance in enumerate(concepts):
    if target[i] == 'Yes':  # Positive example
        for j in range(len(S)):
            if S[j] == 'ϕ':
                S[j] = instance[j]
            elif S[j] != instance[j]:
                S[j] = '?'
        G = [g for g in G if all(g[j] == '?' or g[j] == S[j] for j in range(len(S)))]
    else:  # Negative example
        new_G = []
        for g in G:
            for j in range(len(g)):
                if g[j] == '?' and S[j] != instance[j]:
                    temp = g.copy()
                    temp[j] = S[j]
                    new_G.append(temp)
        G = new_G

    print(f"\nAfter example {i+1}:")
    print("S =", S)
    print("G =", G)

print("\nFinal Version Space:")
print("Specific Boundary (S):", S)
print("General Boundary (G):", G)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Sample credit dataset
data = {
    'Income': [30000, 40000, 50000, 60000, 70000],
    'Age': [22, 25, 35, 45, 52],
    'LoanAmount': [20000, 15000, 30000, 25000, 20000],
    'CreditStatus': [0, 0, 1, 1, 1]   # 0 = Bad, 1 = Good
}

df = pd.DataFrame(data)

X = df[['Income', 'Age', 'LoanAmount']]
y = df['CreditStatus']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Logistic Regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Prediction
y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))

# New customer prediction
new_customer = [[45000, 30, 18000]]
result = model.predict(new_customer)

print("Credit Score:", "Good" if result[0] == 1 else "Bad")

import numpy as np
from sklearn.mixture import GaussianMixture

# Sample data
X = np.array([[1.0], [1.5], [2.0], [6.0], [6.5], [7.0]])

# Gaussian Mixture Model (EM Algorithm)
gmm = GaussianMixture(n_components=2, max_iter=100, random_state=0)
gmm.fit(X)

# Predictions
labels = gmm.predict(X)

print("Cluster assignments:", labels)
print("Means:", gmm.means_)
print("Variances:", gmm.covariances_)

import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error

# Sample data (non-linear)
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
y = np.array([2, 4, 5, 4, 8])

# -------- Linear Regression --------
lin_model = LinearRegression()
lin_model.fit(X, y)
y_lin_pred = lin_model.predict(X)

# -------- Polynomial Regression (degree = 2) --------
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

poly_model = LinearRegression()
poly_model.fit(X_poly, y)
y_poly_pred = poly_model.predict(X_poly)

# Error comparison
print("Linear Regression MSE:", mean_squared_error(y, y_lin_pred))
print("Polynomial Regression MSE:", mean_squared_error(y, y_poly_pred))

import numpy as np
from sklearn.linear_model import LogisticRegression

# Sample dataset
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([0, 0, 0, 1, 1])

# Logistic Regression model
model = LogisticRegression()
model.fit(X, y)

# New input
x_new = np.array([[3]])
prediction = model.predict(x_new)

print("Predicted class:", prediction[0])

import numpy as np

# Input data
X = np.array([1, 2, 3, 4, 5])
Y = np.array([2, 4, 5, 4, 5])

# Mean of X and Y
mean_x = np.mean(X)
mean_y = np.mean(Y)

# Calculate slope (m) and intercept (c)
m = np.sum((X - mean_x) * (Y - mean_y)) / np.sum((X - mean_x)**2)
c = mean_y - m * mean_x

# Prediction
x_new = 6
y_pred = m * x_new + c

print("Slope (m):", m)
print("Intercept (c):", c)
print("Predicted value for x =", x_new, "is", y_pred)

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder

# Dataset
data = {
    'Outlook': ['Sunny','Sunny','Overcast','Rain','Rain','Rain','Overcast','Sunny'],
    'Temperature': ['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild'],
    'Humidity': ['High','High','High','High','Normal','Normal','Normal','High'],
    'Wind': ['Weak','Strong','Weak','Weak','Weak','Strong','Strong','Weak'],
    'PlayTennis': ['No','No','Yes','Yes','Yes','No','Yes','No']
}

df = pd.DataFrame(data)

# Encode categorical values
le = LabelEncoder()
for col in df.columns:
    df[col] = le.fit_transform(df[col])

X = df.drop('PlayTennis', axis=1)
y = df['PlayTennis']

# ID3 Decision Tree
model = DecisionTreeClassifier(criterion='entropy')
model.fit(X, y)

# New sample: Sunny, Cool, High, Strong
new_sample = [[2, 0, 0, 1]]
prediction = model.predict(new_sample)

print("Prediction:", "Yes" if prediction[0] == 1 else "No")

import math
import pandas as pd

def entropy(col):
    values = col.value_counts()
    return sum(-p/len(col) * math.log2(p/len(col)) for p in values)

def info_gain(data, feature, target):
    total = entropy(data[target])
    vals = data[feature].unique()
    return total - sum((len(data[data[feature]==v])/len(data)) *
                       entropy(data[data[feature]==v][target]) for v in vals)

def id3(data, features, target):
    if len(data[target].unique()) == 1:
        return data[target].iloc[0]
    if not features:
        return data[target].mode()[0]

    best = max(features, key=lambda x: info_gain(data, x, target))
    tree = {best: {}}

    for v in data[best].unique():
        subset = data[data[best] == v]
        tree[best][v] = id3(subset, [f for f in features if f != best], target)
    return tree

def predict(tree, sample):
    for k in tree:
        val = sample[k]
        return predict(tree[k][val], sample) if isinstance(tree[k][val], dict) else tree[k][val]

# Dataset
data = {
    'Outlook':['Sunny','Sunny','Overcast','Rain','Rain','Overcast'],
    'Humidity':['High','High','High','Normal','Normal','High'],
    'PlayTennis':['No','No','Yes','Yes','Yes','Yes']
}

df = pd.DataFrame(data)
features = ['Outlook','Humidity']

tree = id3(df, features, 'PlayTennis')
print("Decision Tree:", tree)

sample = {'Outlook':'Sunny','Humidity':'Normal'}
print("Classification:", predict(tree, sample))

import numpy as np

# Sigmoid function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Input and output data
X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([[0],[0],[0],[1]])

# Initialize weights
np.random.seed(1)
w1 = np.random.rand(2,2)
w2 = np.random.rand(2,1)

# Training (Backpropagation)
for _ in range(5000):
    # Forward pass
    h = sigmoid(np.dot(X, w1))
    output = sigmoid(np.dot(h, w2))

    # Backward pass
    error = y - output
    d_output = error * sigmoid_derivative(output)
    d_hidden = d_output.dot(w2.T) * sigmoid_derivative(h)

    # Update weights
    w2 += h.T.dot(d_output)
    w1 += X.T.dot(d_hidden)

# Testing
print("Predicted Output:")
print(np.round(output))

import numpy as np
from scipy.stats import norm

# Sample data
X = np.array([1.0, 1.5, 2.0, 5.0, 6.0, 6.5])

# Initial parameters
mu1, mu2 = 2.0, 6.0
sigma1, sigma2 = 1.0, 1.0
pi1, pi2 = 0.5, 0.5

# EM Algorithm
for _ in range(10):
    # E-step
    r1 = pi1 * norm.pdf(X, mu1, sigma1)
    r2 = pi2 * norm.pdf(X, mu2, sigma2)
    gamma1 = r1 / (r1 + r2)
    gamma2 = r2 / (r1 + r2)

    # M-step
    mu1 = np.sum(gamma1 * X) / np.sum(gamma1)
    mu2 = np.sum(gamma2 * X) / np.sum(gamma2)

    sigma1 = np.sqrt(np.sum(gamma1 * (X - mu1)**2) / np.sum(gamma1))
    sigma2 = np.sqrt(np.sum(gamma2 * (X - mu2)**2) / np.sum(gamma2))

    pi1 = np.mean(gamma1)
    pi2 = np.mean(gamma2)

# Output
print("Cluster 1 Mean:", round(mu1,2))
print("Cluster 2 Mean:", round(mu2,2))

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=1)

# KNN model
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Prediction
y_pred = knn.predict(X_test)

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))

import pandas as pd
from sklearn.linear_model import LinearRegression

# Sample dataset
data = {
    'EngineSize': [1.2, 1.5, 2.0, 2.2, 3.0],
    'Mileage': [25, 20, 15, 12, 10],
    'Price': [500000, 650000, 850000, 950000, 1200000]
}

df = pd.DataFrame(data)

X = df[['EngineSize', 'Mileage']]
y = df['Price']

# Train model
model = LinearRegression()
model.fit(X, y)

# Predict price of a new car
new_car = [[2.0, 18]]
predicted_price = model.predict(new_car)

print("Predicted Car Price:", int(predicted_price[0]))

import pandas as pd
from sklearn.linear_model import LinearRegression

# Dataset
data = {
    'Area': [800, 1000, 1200, 1500, 1800],
    'Bedrooms': [2, 3, 3, 4, 4],
    'Age': [10, 8, 5, 3, 2],
    'Price': [400000, 500000, 650000, 850000, 1000000]
}

df = pd.DataFrame(data)

# Features and target
X = df[['Area', 'Bedrooms', 'Age']]
y = df['Price']

# Train model
model = LinearRegression()
model.fit(X, y)

# Predict price for a new house
new_house = [[1300, 3, 4]]
predicted_price = model.predict(new_house)

print("Predicted House Price:", int(predicted_price[0]))

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Load Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=1)

# Naive Bayes model
model = GaussianNB()
model.fit(X_train, y_train)

# Prediction
y_pred = model.predict(X_test)

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))

import pandas as pd
from sklearn.linear_model import LinearRegression

# Dataset
data = {
    'RAM': [4, 6, 8, 12, 16],
    'Storage': [64, 128, 128, 256, 512],
    'Battery': [4000, 4500, 5000, 5000, 6000],
    'Camera': [12, 48, 64, 108, 108],
    'Price': [12000, 18000, 25000, 45000, 65000]
}

df = pd.DataFrame(data)

X = df[['RAM', 'Storage', 'Battery', 'Camera']]
y = df['Price']

# Train model
model = LinearRegression()
model.fit(X, y)

# Predict price of a new mobile
new_mobile = [[8, 256, 5000, 64]]
predicted_price = model.predict(new_mobile)

print("Predicted Mobile Price: ₹", int(predicted_price[0]))

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Load Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split dataset into training and testing
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42)

# Create Naive Bayes model
model = GaussianNB()
model.fit(X_train, y_train)

# Prediction
y_pred = model.predict(X_test)

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))

import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Sample dataset
data = {
    'Income': ['High','Low','Medium','High','Medium'],
    'CreditScore': ['Good','Poor','Good','Poor','Good'],
    'LoanAmount': ['Low','High','Medium','Medium','Low'],
    'LoanApproved': ['Yes','No','Yes','No','Yes']
}

df = pd.DataFrame(data)

# Convert categorical data to numerical
le = LabelEncoder()
for col in df.columns:
    df[col] = le.fit_transform(df[col])

# Features and target
X = df[['Income','CreditScore','LoanAmount']]
y = df['LoanApproved']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1)

# Naive Bayes model
model = GaussianNB()
model.fit(X_train, y_train)

# Prediction
y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))

import pandas as pd
from sklearn.linear_model import LinearRegression

# Sales dataset
data = {
    'Month': [1, 2, 3, 4, 5, 6],
    'Sales': [120, 135, 150, 165, 180, 200]
}

df = pd.DataFrame(data)

X = df[['Month']]
y = df['Sales']

# Train model
model = LinearRegression()
model.fit(X, y)

# Predict future sales (Month 7)
future_month = [[7]]
future_sales = model.predict(future_month)

print("Predicted Sales for Month 7:", int(future_sales[0]))